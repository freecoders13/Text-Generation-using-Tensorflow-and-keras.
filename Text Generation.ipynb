{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                     Text Generation Project with Poem dataset by Laurence Moroney\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.utils as ku\n",
    "from tensorflow.keras.layers import Dense, Flatten, Bidirectional, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('sonnets.txt').read()  #Reading data file\n",
    "corpus = data.lower().split('\\n')  #Converting the alphabets to small and splitting data from where '\\n' appear\n",
    "\n",
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts(corpus)  #feeding vocab to the model\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  #converting text to the sequences\n",
    "    for i in range(1 , len(token_list)):\n",
    "        n_gram_sequences = token_list[:i+1]  #converting token list to n_gram_sequences\n",
    "        input_sequences.append(n_gram_sequences)  #feeding N-gram_sequences to a separate list named as 'input_sequences'\n",
    "\n",
    "maximum_len = max([len(x) for x in input_sequences])  #getting length of the biggest sentence in the 'input_sequences'\n",
    "input_sequences = pad_sequences(input_sequences , maxlen = maximum_len , padding = 'pre')  #pre_padding the 'input_sequences'\n",
    "\n",
    "predictors , label = input_sequences[:,:-1],input_sequences[:,-1]  #getting labels and predictors for the 'input_sequences'\n",
    "label = ku.to_categorical(label , num_classes = total_words)  #labelling entire vocabullary uniquely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12038 samples\n",
      "Epoch 1/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 6.8414 - accuracy: 0.0639\n",
      "Epoch 2/100\n",
      "12038/12038 [==============================] - 54s 5ms/sample - loss: 6.3066 - accuracy: 0.0663\n",
      "Epoch 3/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 6.1310 - accuracy: 0.0681\n",
      "Epoch 4/100\n",
      "12038/12038 [==============================] - 51s 4ms/sample - loss: 6.0138 - accuracy: 0.0774\n",
      "Epoch 5/100\n",
      "12038/12038 [==============================] - 51s 4ms/sample - loss: 5.9022 - accuracy: 0.0811\n",
      "Epoch 6/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 5.7751 - accuracy: 0.0930\n",
      "Epoch 7/100\n",
      "12038/12038 [==============================] - 51s 4ms/sample - loss: 5.6562 - accuracy: 0.1012\n",
      "Epoch 8/100\n",
      "12038/12038 [==============================] - 47s 4ms/sample - loss: 5.5580 - accuracy: 0.1080\n",
      "Epoch 9/100\n",
      "12038/12038 [==============================] - 51s 4ms/sample - loss: 5.4836 - accuracy: 0.1102\n",
      "Epoch 10/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 5.4040 - accuracy: 0.1170\n",
      "Epoch 11/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 5.3403 - accuracy: 0.1174\n",
      "Epoch 12/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 5.2689 - accuracy: 0.1213\n",
      "Epoch 13/100\n",
      "12038/12038 [==============================] - 51s 4ms/sample - loss: 5.1987 - accuracy: 0.1257\n",
      "Epoch 14/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 5.1341 - accuracy: 0.1284\n",
      "Epoch 15/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 5.0670 - accuracy: 0.1303\n",
      "Epoch 16/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 4.9875 - accuracy: 0.1352\n",
      "Epoch 17/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 4.9169 - accuracy: 0.1376\n",
      "Epoch 18/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 4.8456 - accuracy: 0.1425\n",
      "Epoch 19/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 4.7721 - accuracy: 0.1460\n",
      "Epoch 20/100\n",
      "12038/12038 [==============================] - 50s 4ms/sample - loss: 4.6944 - accuracy: 0.1534\n",
      "Epoch 21/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 4.6237 - accuracy: 0.1585\n",
      "Epoch 22/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 4.5412 - accuracy: 0.1671\n",
      "Epoch 23/100\n",
      "12038/12038 [==============================] - 59s 5ms/sample - loss: 4.4719 - accuracy: 0.1713\n",
      "Epoch 24/100\n",
      "12038/12038 [==============================] - 57s 5ms/sample - loss: 4.3981 - accuracy: 0.1803\n",
      "Epoch 25/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 4.3252 - accuracy: 0.1886\n",
      "Epoch 26/100\n",
      "12038/12038 [==============================] - 56s 5ms/sample - loss: 4.2567 - accuracy: 0.1936\n",
      "Epoch 27/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 4.1951 - accuracy: 0.1993\n",
      "Epoch 28/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 4.1212 - accuracy: 0.2065\n",
      "Epoch 29/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 4.0682 - accuracy: 0.2137\n",
      "Epoch 30/100\n",
      "12038/12038 [==============================] - 57s 5ms/sample - loss: 3.9997 - accuracy: 0.2221\n",
      "Epoch 31/100\n",
      "12038/12038 [==============================] - 59s 5ms/sample - loss: 3.9293 - accuracy: 0.2293\n",
      "Epoch 32/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 3.8578 - accuracy: 0.2413\n",
      "Epoch 33/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 3.8005 - accuracy: 0.2465\n",
      "Epoch 34/100\n",
      "12038/12038 [==============================] - 70s 6ms/sample - loss: 3.7296 - accuracy: 0.2593\n",
      "Epoch 35/100\n",
      "12038/12038 [==============================] - 65s 5ms/sample - loss: 3.6761 - accuracy: 0.2665\n",
      "Epoch 36/100\n",
      "12038/12038 [==============================] - 67s 6ms/sample - loss: 3.6199 - accuracy: 0.2797s - l\n",
      "Epoch 37/100\n",
      "12038/12038 [==============================] - 68s 6ms/sample - loss: 3.5957 - accuracy: 0.2822s - loss: 3.5958 - ac\n",
      "Epoch 38/100\n",
      "12038/12038 [==============================] - 69s 6ms/sample - loss: 3.5189 - accuracy: 0.2932\n",
      "Epoch 39/100\n",
      "12038/12038 [==============================] - 63s 5ms/sample - loss: 3.4486 - accuracy: 0.3143\n",
      "Epoch 40/100\n",
      "12038/12038 [==============================] - 62s 5ms/sample - loss: 3.3895 - accuracy: 0.3249\n",
      "Epoch 41/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 3.3381 - accuracy: 0.3341\n",
      "Epoch 42/100\n",
      "12038/12038 [==============================] - 64s 5ms/sample - loss: 3.2881 - accuracy: 0.3454\n",
      "Epoch 43/100\n",
      "12038/12038 [==============================] - 62s 5ms/sample - loss: 3.2403 - accuracy: 0.3546\n",
      "Epoch 44/100\n",
      "12038/12038 [==============================] - 62s 5ms/sample - loss: 3.1906 - accuracy: 0.3668\n",
      "Epoch 45/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 3.1418 - accuracy: 0.3749\n",
      "Epoch 46/100\n",
      "12038/12038 [==============================] - 59s 5ms/sample - loss: 3.0959 - accuracy: 0.3902\n",
      "Epoch 47/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 3.0447 - accuracy: 0.3953\n",
      "Epoch 48/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 3.0000 - accuracy: 0.4090\n",
      "Epoch 49/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.9518 - accuracy: 0.4190\n",
      "Epoch 50/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.9026 - accuracy: 0.4311\n",
      "Epoch 51/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.8591 - accuracy: 0.4404\n",
      "Epoch 52/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 2.8212 - accuracy: 0.4473\n",
      "Epoch 53/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 2.7749 - accuracy: 0.4549\n",
      "Epoch 54/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.7433 - accuracy: 0.4630\n",
      "Epoch 55/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 2.6931 - accuracy: 0.4768\n",
      "Epoch 56/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.6474 - accuracy: 0.4864\n",
      "Epoch 57/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 2.6278 - accuracy: 0.4900\n",
      "Epoch 58/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.5796 - accuracy: 0.4998\n",
      "Epoch 59/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 2.5543 - accuracy: 0.5086\n",
      "Epoch 60/100\n",
      "12038/12038 [==============================] - 56s 5ms/sample - loss: 2.5074 - accuracy: 0.5124\n",
      "Epoch 61/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.4767 - accuracy: 0.5248\n",
      "Epoch 62/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.4299 - accuracy: 0.5309\n",
      "Epoch 63/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 2.4005 - accuracy: 0.5402\n",
      "Epoch 64/100\n",
      "12038/12038 [==============================] - 59s 5ms/sample - loss: 2.3680 - accuracy: 0.5477\n",
      "Epoch 65/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 2.3311 - accuracy: 0.5570\n",
      "Epoch 66/100\n",
      "12038/12038 [==============================] - 56s 5ms/sample - loss: 2.3155 - accuracy: 0.5541\n",
      "Epoch 67/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.2767 - accuracy: 0.5693\n",
      "Epoch 68/100\n",
      "12038/12038 [==============================] - 57s 5ms/sample - loss: 2.2440 - accuracy: 0.5746\n",
      "Epoch 69/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.2145 - accuracy: 0.5772\n",
      "Epoch 70/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.1878 - accuracy: 0.5910\n",
      "Epoch 71/100\n",
      "12038/12038 [==============================] - 54s 5ms/sample - loss: 2.1603 - accuracy: 0.5917\n",
      "Epoch 72/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 2.1305 - accuracy: 0.6003\n",
      "Epoch 73/100\n",
      "12038/12038 [==============================] - 56s 5ms/sample - loss: 2.1047 - accuracy: 0.6028\n",
      "Epoch 74/100\n",
      "12038/12038 [==============================] - 58s 5ms/sample - loss: 2.0915 - accuracy: 0.6116\n",
      "Epoch 75/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.0644 - accuracy: 0.6142\n",
      "Epoch 76/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 2.0246 - accuracy: 0.6187\n",
      "Epoch 77/100\n",
      "12038/12038 [==============================] - 53s 4ms/sample - loss: 1.9897 - accuracy: 0.6313\n",
      "Epoch 78/100\n",
      "12038/12038 [==============================] - 57s 5ms/sample - loss: 1.9765 - accuracy: 0.6300\n",
      "Epoch 79/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 1.9571 - accuracy: 0.6367\n",
      "Epoch 80/100\n",
      "12038/12038 [==============================] - 62s 5ms/sample - loss: 1.9434 - accuracy: 0.6434\n",
      "Epoch 81/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 1.9180 - accuracy: 0.6490\n",
      "Epoch 82/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 1.8999 - accuracy: 0.6499\n",
      "Epoch 83/100\n",
      "12038/12038 [==============================] - 62s 5ms/sample - loss: 1.8811 - accuracy: 0.6501\n",
      "Epoch 84/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 1.8369 - accuracy: 0.6615\n",
      "Epoch 85/100\n",
      "12038/12038 [==============================] - 63s 5ms/sample - loss: 1.8179 - accuracy: 0.6700\n",
      "Epoch 86/100\n",
      "12038/12038 [==============================] - 64s 5ms/sample - loss: 1.8073 - accuracy: 0.6698\n",
      "Epoch 87/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 1.7981 - accuracy: 0.6674\n",
      "Epoch 88/100\n",
      "12038/12038 [==============================] - 59s 5ms/sample - loss: 1.7684 - accuracy: 0.6746\n",
      "Epoch 89/100\n",
      "12038/12038 [==============================] - 60s 5ms/sample - loss: 1.7499 - accuracy: 0.6802\n",
      "Epoch 90/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 1.7246 - accuracy: 0.6874\n",
      "Epoch 91/100\n",
      "12038/12038 [==============================] - 65s 5ms/sample - loss: 1.7210 - accuracy: 0.6838\n",
      "Epoch 92/100\n",
      "12038/12038 [==============================] - 57s 5ms/sample - loss: 1.6903 - accuracy: 0.6906\n",
      "Epoch 93/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 1.6777 - accuracy: 0.6946\n",
      "Epoch 94/100\n",
      "12038/12038 [==============================] - 61s 5ms/sample - loss: 1.6622 - accuracy: 0.6960\n",
      "Epoch 95/100\n",
      "12038/12038 [==============================] - 54s 5ms/sample - loss: 1.6436 - accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "12038/12038 [==============================] - 54s 4ms/sample - loss: 1.6258 - accuracy: 0.7074\n",
      "Epoch 97/100\n",
      "12038/12038 [==============================] - 56s 5ms/sample - loss: 1.6127 - accuracy: 0.7073\n",
      "Epoch 98/100\n",
      "12038/12038 [==============================] - 55s 5ms/sample - loss: 1.5954 - accuracy: 0.7102\n",
      "Epoch 99/100\n",
      "12038/12038 [==============================] - 52s 4ms/sample - loss: 1.5822 - accuracy: 0.7117\n",
      "Epoch 100/100\n",
      "12038/12038 [==============================] - 48s 4ms/sample - loss: 1.5659 - accuracy: 0.7144\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  \n",
    "\n",
    "model.add(Embedding(total_words , 100 , input_length = maximum_len-1))  #Embedding with '100' dims\n",
    "model.add(Bidirectional(LSTM(150 , return_sequences  = True)))  #'LSTM' with '150' units.\n",
    "model.add(Dropout(0.2))  #dropping '20%' of the neurons.\n",
    "model.add(LSTM(100))  #'LSTM' of 100 units.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(total_words/2.00 ,activation =  'relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words , activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(predictors , label , epochs = 100 , verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me Obi Wan Kenobi, you're my only hope forever as he gave to me a whirligig that no tenant thou with captain pepper as his fathers barrow till she well tell me home to me to care you love my souls william beguile agree on your breast in the grass he said no tight thou him down and rest to me very as hit left me contented now in my saber forever it doesnt we sat course it had a stone thee clung of the sky above i think i will cease to sip trod the pale sands the cliffs of doneen no black song on the bridge\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"  #using this sentence to generate a poem\n",
    "next_words = 100  #generating '100' words of poem\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=maximum_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():  #looping through words and indices of seed text.\n",
    "        if index == predicted:  #condition to get predicted index equal to current index, to use it for next predictions.\n",
    "            output_word = word  #condition to get output word equal to current word, to use it for next predictions.\n",
    "            break\n",
    "    seed_text += \" \" + output_word  #joining seed text with next predicted text.\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
